# Многопоточный парсер (threading_parser.py)

## Описание

Многопоточный парсер использует модуль `threading` и `ThreadPoolExecutor` для создания нескольких потоков, которые обрабатывают URL параллельно.

## Используемые методы

- **threading**: Основной модуль для работы с потоками
- **ThreadPoolExecutor**: Управление пулом потоков
- **concurrent.futures**: Высокоуровневый интерфейс для асинхронного выполнения
- **requests**: Синхронные HTTP-запросы
- **BeautifulSoup**: Парсинг HTML/XML контента

## Особенности реализации

```python
# Создание пула потоков
with ThreadPoolExecutor(max_workers=THREADING_WORKERS) as executor:
    # Разделение URL на блоки для потоков
    url_chunks = [URLS[i:i + chunk_size] for i in range(0, len(URLS), chunk_size)]
    
    # Запуск задач в потоках
    futures = [executor.submit(worker, chunk) for chunk in url_chunks]
    
    # Ожидание завершения всех задач
    for future in futures:
        future.result()
```

## Технические характеристики

| Характеристика | Значение |
|----------------|----------|
| Время выполнения | 5.48 секунды |
| Успешность | 100% (10/10) |
| Количество потоков | 5 рабочих потоков |
| Использование памяти | Среднее |
| Переключение контекста | Минимальное |

## Преимущества

**100% надежность** - все URL обработаны успешно  
**Простота понимания** - синхронный код легче отлаживать  
**Общая память** - потоки разделяют память процесса  
**Низкие накладные расходы** - быстрое переключение потоков  

## Недостатки

**Ограничения GIL** - Python Global Interpreter Lock  
**Медленнее асинхронного** - 5.48 секунды vs 3.34 секунды  
**Не подходит для CPU-интенсивных задач** - из-за GIL  

## Применение

Многопоточный парсер идеально подходит для:
- Задач с высокими требованиями к надежности
- I/O-интенсивных операций
- Систем с ограниченными ресурсами
- Проектов, где важна простота кода
