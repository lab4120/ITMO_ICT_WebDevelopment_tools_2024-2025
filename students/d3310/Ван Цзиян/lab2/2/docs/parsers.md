# Парсеры веб-страниц

Этот раздел содержит подробную документацию по трем различным методам параллельного парсинга веб-страниц.

## Асинхронный парсер

### Описание

Асинхронный парсер использует `asyncio` и `aiohttp` для реализации неблокирующего ввода-вывода. Все задачи выполняются в одном потоке через цикл событий.

### Используемые методы

- **asyncio**: Основная библиотека для асинхронного программирования
- **aiohttp**: Асинхронная HTTP-клиентская библиотека
- **asyncio.Semaphore**: Контроль количества конкурентных операций
- **BeautifulSoup**: Парсинг HTML/XML контента

### Особенности реализации

```python
# Создание семафора для контроля конкурентности
semaphore = asyncio.Semaphore(ASYNC_CONCURRENCY)

# Асинхронная функция парсинга
async def parse_and_save(url: str, session: aiohttp.ClientSession, semaphore: asyncio.Semaphore):
    async with semaphore:
        # Асинхронный HTTP-запрос
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
            content = await response.read()
        
        # Парсинг контента
        soup = BeautifulSoup(content, 'html.parser')
        title = soup.find('title')
```

### Технические характеристики

| Характеристика | Значение |
|----------------|----------|
| Время выполнения | 3.34 секунды |
| Успешность | 100% (10/10) |
| Конкурентность | 10 одновременных соединений |
| Использование памяти | Минимальное |
| Переключение контекста | Отсутствует |

### Преимущества

**Самая высокая производительность** - 3.34 секунды  
**Минимальное использование ресурсов** - один поток  
**Отсутствие накладных расходов на переключение**  
**Подходит для большого количества I/O операций**  

### Недостатки

**Сложность отладки** - асинхронный код сложнее для понимания  
**Зависимость от библиотек** - требует aiohttp  
**Ограничения совместимости** - не все библиотеки поддерживают async/await  

### Применение

Асинхронный парсер идеально подходит для:
- Массового парсинга веб-страниц
- API-запросов
- Задач с интенсивным I/O
- Систем с ограниченными ресурсами

---

## Многопоточный парсер

### Описание

Многопоточный парсер использует модуль `threading` и `ThreadPoolExecutor` для создания нескольких потоков, которые обрабатывают URL параллельно.

### Используемые методы

- **threading**: Основной модуль для работы с потоками
- **ThreadPoolExecutor**: Управление пулом потоков
- **concurrent.futures**: Высокоуровневый интерфейс для асинхронного выполнения
- **requests**: Синхронные HTTP-запросы
- **BeautifulSoup**: Парсинг HTML/XML контента

### Особенности реализации

```python
# Создание пула потоков
with ThreadPoolExecutor(max_workers=THREADING_WORKERS) as executor:
    # Разделение URL на блоки для потоков
    url_chunks = [URLS[i:i + chunk_size] for i in range(0, len(URLS), chunk_size)]
    
    # Запуск задач в потоках
    futures = [executor.submit(worker, chunk) for chunk in url_chunks]
    
    # Ожидание завершения всех задач
    for future in futures:
        future.result()
```

### Технические характеристики

| Характеристика | Значение |
|----------------|----------|
| Время выполнения | 5.48 секунды |
| Успешность | 100% (10/10) |
| Количество потоков | 5 рабочих потоков |
| Использование памяти | Среднее |
| Переключение контекста | Минимальное |

### Преимущества

**100% надежность** - все URL обработаны успешно  
**Простота понимания** - синхронный код легче отлаживать  
**Общая память** - потоки разделяют память процесса  
**Низкие накладные расходы** - быстрое переключение потоков  

### Недостатки

**Ограничения GIL** - Python Global Interpreter Lock  
**Медленнее асинхронного** - 5.48 секунды vs 3.34 секунды  
**Не подходит для CPU-интенсивных задач** - из-за GIL  

### Применение

Многопоточный парсер идеально подходит для:
- Задач с высокими требованиями к надежности
- I/O-интенсивных операций
- Систем с ограниченными ресурсами
- Проектов, где важна простота кода

---

## Многопроцессный парсер

### Описание

Многопроцессный парсер использует модуль `multiprocessing` для создания нескольких независимых процессов, каждый из которых обрабатывает URL параллельно.

### Используемые методы

- **multiprocessing**: Основной модуль для работы с процессами
- **Pool**: Управление пулом процессов
- **imap_unordered**: Неупорядоченное выполнение задач для повышения эффективности
- **requests**: Синхронные HTTP-запросы
- **BeautifulSoup**: Парсинг HTML/XML контента

### Особенности реализации

```python
# Создание пула процессов
with Pool(processes=MULTIPROCESSING_WORKERS) as pool:
    # Неупорядоченное выполнение для повышения эффективности
    results = list(pool.imap_unordered(parse_and_save, URLS))

# Каждый процесс создает собственное подключение к БД
def parse_and_save(url: str):
    db_manager = DatabaseManager()
    try:
        db_manager.connect()
        # Обработка URL
    finally:
        db_manager.close()
```

### Технические характеристики

| Характеристика | Значение |
|----------------|----------|
| Время выполнения | 4.98 секунды |
| Успешность | 90% (9/10) |
| Количество процессов | 8 рабочих процессов |
| Использование памяти | Высокое |
| Переключение контекста | Значительное |

### Преимущества

**Обход GIL** - истинное параллельное выполнение  
**Изоляция процессов** - каждый процесс имеет независимую память  
**Высокая производительность** - 4.98 секунды  
**Подходит для CPU-интенсивных задач** - нет ограничений GIL  

### Недостатки

**Высокие накладные расходы** - создание процессов дорого  
**Большое потребление памяти** - каждый процесс имеет собственную память  
**Сложность коммуникации** - процессы не разделяют память  
**90% успешность** - один URL не обработан из-за сетевой ошибки  

### Применение

Многопроцессный парсер идеально подходит для:
- CPU-интенсивных вычислений
- Задач, требующих обхода GIL
- Систем с множественными ядрами
- Критически важных по производительности приложений
